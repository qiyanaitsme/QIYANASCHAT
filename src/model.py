from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import logging
from tqdm import tqdm
import time
import re


class QIYANAModel:
    def __init__(self, model_path):
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self.load_model()

    def load_model(self):
        print("\n" + "=" * 50)
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è QIYANAChat")
        print("=" * 50)

        steps = ['–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞', '–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏', '–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤']
        for step in tqdm(steps, desc="üí´ –ü—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–≥—Ä—É–∑–∫–∏"):
            if '—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞' in step:
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            elif '–º–æ–¥–µ–ª–∏' in step:
                self.model = AutoModelForCausalLM.from_pretrained(self.model_path, trust_remote_code=True)
            time.sleep(1)

    def generate_response(self, prompt):
        if not prompt.strip():
            return "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ."

        logging.info(f"Processing: {prompt[:50]}...")

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt")
            outputs = self.model.generate(**inputs, max_length=200)
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            return response
        except Exception as e:
            logging.error(f"Error: {str(e)}")
            return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å."